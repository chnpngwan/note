##### 笔记

- Union： 【Spark中的并集】并集需要两个集合的数据泛型一样

##### 累加器模型

##### 空指针异常

- 调用一个空（null）对象的成员属性或者成员方法的时候，会发生空指针异常

##### wait()和sleep()区别

- sleep()是静态方法，和类有关系，main线程在休眠，
- wait（）是成员方法，和对象有关系，

##### Spark算子 -- 3

![1658534862305](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658534862305.png)

![1658534876014](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658534876014.png)

![1658534890173](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658534890173.png)

![1658534918823](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658534918823.png)

##### WordCount 的10中方式

```scala
package wordcount

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable

object Spark01_WordCount10 {
  def main(args: Array[String]): Unit = {
    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName("SparkCoreTest").setMaster("local[*]")

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)


    val rdd1: RDD[String] = sc.makeRDD(
      List("a", "a", "b", "b", "c", "c")
    )

    // 01：groupBy + mapvalues -----------------------------
    val rdd11: RDD[(String, Iterable[String])] = rdd1.groupBy(word => word)
    val rdd12: RDD[(String, Int)] = rdd11.mapValues(Iterq => Iterq.size)
    rdd12.collect().foreach(println)

    // 02： countByvalue =====================================
    val rdd21: collection.Map[String, Long] = rdd1.countByValue()


    // 03：  groupByKey + mapValues-------------------------------
    val rdd3 = sc.makeRDD(
      List(
        ("a", 1), ("a", 1), ("a", 1)
      )
    )

    val rdd31: RDD[(String, Iterable[Int])] = rdd3.groupByKey()
    val rdd32: RDD[(String, Int)] = rdd31.mapValues(_.size)


    // 04： reduceByKey-------------------------------
    val rdd41: RDD[(String, Int)] = rdd3.reduceByKey(_ + _)

    // 05： countByKey------------------------------------
    val rdd51: collection.Map[String, Long] = rdd3.countByKey()

    // 06： aggregateByKey(0)(_ + _, _ + _)-----------------------
    val rdd4 = sc.makeRDD(
      List(
        ("a", 1), ("a", 2), ("a", 3)
      )
    )

    val rdd61: RDD[(String, Int)] = rdd4.aggregateByKey(0)(_ + _, _ + _)

    // 07： foldByKey(0)(_ + _)------------------------------
    val rdd71: RDD[(String, Int)] = rdd4.foldByKey(0)(_ + _)

    // 08： combineByKey ---------------------------------------
    val rdd81: RDD[(String, Int)] = rdd4.combineByKey(
      v => v,
      (v1: Int, v2) => {
        v1 + v2
      },
      (v1: Int, v2: Int) => {
        v1 + v2
      }
    )

    // 数据转换为Map 类型之后使用以下三个方法
    // -09： dd5.reduce  ---------------------------------
    val rdd5: RDD[mutable.Map[String, Int]] = rdd4.map(
      kv => {
        val stringToInt: mutable.Map[String, Int] = mutable.Map(kv)
        stringToInt
      }
    )

    //    rdd4.map().reduce()
    val rdd9: mutable.Map[String, Int] = rdd5.reduce(
      (map1, map2) => {
        map2.foreach {
          case (word, cnt) => {
            val oldCnt: Int = map1.getOrElse(word, 0)
            map1.update(word, cnt + oldCnt)
          }
        }
        map1
      }
    )
    //   10：  ------------------------------ rdd4.aggregate()
    val rdd101: mutable.Map[String, Int] = rdd5.aggregate(mutable.Map[String, Int]())(
      (map1, map2) => {
        map2.foreach {
          case (word, cnt) => {
            val oldCnt: Int = map1.getOrElse(word, 0)
            map1.update(word, oldCnt + cnt)
          }
        }
        map1
      },
      (map1, map2) => {
        map2.foreach {
          case (word, cnt) => {
            val oldCnt: Int = map1.getOrElse(word, 0)
            map1.update(word, oldCnt + cnt)
          }
        }
        map1
      }
    )
    //    11： -----------------------------------------------rdd4.fold()
    val rdd111: mutable.Map[String, Int] = rdd5.fold(mutable.Map[String, Int]()) {
      (map1, map2) => {
        map2.foreach {
          case (word, cnt) => {
            val oldCnt: Int = map1.getOrElse(word, 0)
            map1.update(word, oldCnt + cnt)
          }
        }
        map1
      }
    }

    //4.关闭连接
    sc.stop()
  }
}
```

##### 累加器使用

- sc.longAccumulator("sum")
- 算子内部使用到了外部的 sum， 产生了闭包环境， 序列化， Spark 将数据拉取到了 Executer 端，
- RDD 的算子是分布式的， 当算子的内部使用到了外部变脸， 会将变量拉取到 Executer 端，
- 需要使用累加器次才能将变量重新拉回到Driver 端， 完成聚合

```scala
package rdd.acc

import org.apache.spark.rdd.RDD
import org.apache.spark.util.LongAccumulator
import org.apache.spark.{SparkConf, SparkContext}

// 练习代码
object Spark02_Acc {
  def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName("SparkCoreTest").setMaster("local[*]")

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    val rdd0: RDD[Int] = sc.makeRDD(
      List(1, 2, 3, 4),
      2

    )

    // TODO 1.声明累加器模型
    val sum: LongAccumulator = sc.longAccumulator("sum")

    // sc.doubleAccumulator("sum1")
    // sc.collectionAccumulator()

    rdd0.foreach(
      num => {

        // TODO 2.累加数据
        sum.add(num)
      }
    )

    // TODO 3.获取累加器的结果
    println(sum.value)



    //4.关闭连接
    sc.stop()
  }
}
```

##### 自定义累加器实现WordCount

```scala
package rdd.acc

import org.apache.spark.util.AccumulatorV2
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable

// 练习代码
object Spark04_Acc_WordCount {
  def main(args: Array[String]): Unit = {

    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName("SparkCoreTest").setMaster("local[*]")

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    val rdd0 = sc.makeRDD(
      List(
        "a", "a", "a",
        "b", "b", "c"
      )

    )

    // TODO 使用累加器实现 WordCount
    //    1.构建模型
    val acc = new WordCountAcc()

    //    2.向Spark 注册
    sc.register(acc)
    //    3.使用模型
    rdd0.foreach(
      word => {
        acc.add(word)
      }
    )
    //    4.获取模型的值；
    val wordCount: mutable.Map[String, Int] = acc.value
    println(wordCount)

    //4.关闭连接
    sc.stop()
  }

  // TODO  自定义累加器，实现 WordCount
  //    1. 继承 AccumulatorV2
  //    2. 定义泛型
  //      IN: String
  //      OUT: Map[String, Interger]
  //    2. 重写方法（）；
  class WordCountAcc extends AccumulatorV2[String, mutable.Map[String, Int]] {
    override def isZero: Boolean = {
      wcMap.isEmpty
    }

    override def copy(): AccumulatorV2[String, mutable.Map[String, Int]] = {
      new WordCountAcc
    }

    override def reset(): Unit = {
      wcMap.clear()
    }

    override def add(v: String): Unit = {
      val oldCount: Int = wcMap.getOrElse(v, 0)
      wcMap.update(v, oldCount + 1)
    }

    override def merge(other: AccumulatorV2[String, mutable.Map[String, Int]]): Unit = {
      other.value.foreach{
        case (word, otherCount) =>{
          val oldCount: Int = wcMap.getOrElse(word, 0)
          wcMap.update(word, oldCount + otherCount)
        }
      }
    }

    override def value: mutable.Map[String, Int] = {
      wcMap
    }

    private val wcMap = mutable.Map[String, Int]()
  }

}
```

##### 自定义累加器实现TOP10

```scala
package req

import org.apache.spark.rdd.RDD
import org.apache.spark.util.AccumulatorV2
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable
import scala.collection.mutable.ArrayOps

// 练习代码
object Spark04_Req {
  def main(args: Array[String]): Unit = {
    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName("SparkCoreTest").setMaster("local[*]")

    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)

    // TODO 读取数据， 获取原始数据
    val datasRDD: RDD[String] = sc.textFile("data/user_visit_action.txt")

    // TODO 重复使用的 RDD 尽量采用持久化操作
    datasRDD.cache()
    // TODO 使用累加器实现统计聚合

    // 1.模型构建
    val acc = new HotCategoryACC

    // 2.注册
    sc.register(acc)

    datasRDD.foreach(
      data =>{
        // 3. 累加数据
        val datas: Array[String] = data.split("_")

        if (datas(6) != "-1"){
          acc.add(datas(6), "click")
        } else if (datas(8) != "null"){
          datas(8).split(",").foreach(
            id =>{
              acc.add(id, "order")
            }
          )
        } else if (datas(10) != "null"){
          datas(10).split(",").foreach(
            id =>{
              acc.add(id, "pay")
            }
          )
        }
      }
    )

    // 4. 获取结果 MAP(品类， （点击， 下单， 支付）)
    val accMap: mutable.Map[String, HotCategryCount] = acc.value

    val list: List[HotCategryCount] = accMap.map(_._2).toList

    list.sortWith(
      (c1, c2) =>{
        if (c1.clinCnt > c2.clinCnt){
          true
        } else if (c1.clinCnt == c2.clinCnt){
          if (c1.orderCount > c2.orderCount){
            true
          } else if (c1.orderCount == c2.orderCount){
            c1.payCount > c2.payCount
          } else {
            false
          }
        } else {
          false
        }
      }
    ).take(10).foreach(println)


    //4.关闭连接
    sc.stop()
  }

  // TODO 样例类中的构造参数 会自动作为类的属性存在， 不能修改
  //    想要修改需要设定为 var ；
  case class HotCategryCount(var cid:String, var clinCnt: Int, var orderCount:Int, var payCount: Int)

  // TODO 热门品类统计
  // 1. 继承类 AccumulatorV2
  // 2. 定义泛型
  //  IN: (Sreing, String)  (品类， 行为类型)
  //  OUT: MAP(品类， （点击， 下单， 支付）);
  class HotCategoryACC extends AccumulatorV2[(String, String), mutable.Map[String, HotCategryCount]]{
    override def isZero: Boolean = {
      hcMap.isEmpty
    }

    override def copy(): AccumulatorV2[(String, String), mutable.Map[String, HotCategryCount]] = {
      new HotCategoryACC
    }

    override def reset(): Unit = {
      hcMap.clear()
    }

    override def add(v: (String, String)): Unit = {
      val (cid, actionType) = v

      val oldCount: HotCategryCount = hcMap.getOrElse(cid, HotCategryCount(cid, 0, 0, 0))

      actionType match {
        case "click" => oldCount.clinCnt +=1
        case "order" => oldCount.orderCount += 1
        case "pay" => oldCount.payCount += 1

      }
      hcMap.update(cid, oldCount)

    }

    override def merge(other: AccumulatorV2[(String, String), mutable.Map[String, HotCategryCount]]): Unit = {
      other.value.foreach{
        case (cid, otherCnt) =>{
          val thisCnt: HotCategryCount = hcMap.getOrElse(cid, HotCategryCount(cid, 0, 0, 0))
          thisCnt.clinCnt += otherCnt.clinCnt
          thisCnt.orderCount += otherCnt.orderCount
          thisCnt.payCount += otherCnt.payCount

          hcMap.update(cid, thisCnt)

        }


      }
    }

    override def value: mutable.Map[String, HotCategryCount] = {
      hcMap
    }

    private val hcMap = mutable.Map[String, HotCategryCount]()
  }
}

```

##### 广播变量

```scala
package com.atguigu.bigdata.spark.core.acc

import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.util.AccumulatorV2
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable

object Spark06_Acc {

    def main(args: Array[String]): Unit = {

        val conf = new SparkConf().setMaster("local[*]").setAppName("Acc")
        val sc = new SparkContext(conf);

        val rdd1 = sc.makeRDD(
            List(
                ("a", 1), ("b", 2)
            )
        )

        val rdd2 = sc.makeRDD(
            List(
                ("a", 3), ("b", 4)
            )
        )

        val map = Map( ("a", 3), ("b", 4) )
        val bc: Broadcast[Map[String, Int]] = sc.broadcast(map)

        rdd1.map {
            case ( word, cnt) => {
                (word, (cnt, bc.value.getOrElse(word, 0)))
            }
        }.foreach(println)

        //joinRDD.foreach(println)

        sc.stop()
    }
}
```

