##### 	笔记

1. 栈溢出：线程个数相关
2. 栈内存溢出： 压栈的时候的，再来一个线程的时候，没有足够的内存分配给他，线程不能运行
3. 栈内存： 线程独享， 每个线程一个栈内存

**闭包：一个函数使用了外部的变量，改变这个变量的生命周期，将变量包含到函数的内部，形成闭合的环境，简称为闭包**

- Kyro：序列化框架， 绕过了Java中的默认序列化规则

- `transient`: 关键字， 使用这个关键字修饰的属性，序列化的时候不会被序列化

##### RDD五个特性

![1658275859582](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658275859582.png)

##### 广告点击案例优化

1. 简化数据，分省+广告ID进行reduce
2. 转换数据格式
3. 分区计算以及统计，（增加判断，确认数据的条数小于等于 3 ）

```scala

    // TODO 省份  + 点击广告(WordCount)   TOP3
    //  一行：时间戳 省份 城市 用户 广告;
    //          0     1   2   3    4

    // TODO 先聚合再分组
    //  先统计广告点击次数， 再按照省份分组；


    //    TODO 1. duqu 读数据
    val lineDatas: RDD[String] = sc.textFile("data/data.txt")

    val reduceRDD: RDD[((String, String), Int)] = lineDatas.map(
      line => {
        val datas: Array[String] = line.split(" ")
        ((datas(1), datas(4)), 1)
      }
    ).reduceByKey(_ + _)

    // 将统计结果转换结构后 按照省份分组
	// 处理的是各个 相同的key值的 value
    val mapRDD: RDD[(String, (String, Int))] = reduceRDD.map {
      case ((prv, ad), cnt) => {
        (prv, (ad, cnt))
      }
    }
    val top3: RDD[(String, ListBuffer[(String, Int)])] = mapRDD.aggregateByKey(ListBuffer[(String, Int)]())(
      (list, kv) => { // 分区内计算，使用了默认值与分区之内的各个元素计算

        if (list.size < 3) {
          list.append(kv)
          list
        } else {
          list.append(kv)
          list.sortBy(_._2)(Ordering.Int.reverse).take(3)
        }
      },
      (list1, list2) => { // 分区之间计算
        list1.appendAll(list2)
        list1.sortBy(_._2)(Ordering.Int.reverse).take(3)
      }
    )

    top3.collect().foreach(println)

  }
}
```

##### joinRDD

1. 相同的 key 连接在一起 格式 `(String, (Int, Int))`
2. 性能比较差， 能不用尽量不用， 可能有shuffle 可能有笛卡尔乘积；

```scala
    val rdd1: RDD[(String, Int)] = sc.makeRDD(
      List(
        ("a", 4), ("d", 7), ("a", 3), ("c", 1)
      )
    )

    val rdd2: RDD[(String, Int)] = sc.makeRDD(
      List(
        ("a", 1), ("a", 4), ("b", 8), ("c", 9)
      )
    )

    // JOIN 将两张表连接到一起
    // Spark 中连接的条件是： 将相同的key连接在一起，
    //    如果没有相同的key 无法连接, 数据会被忽略
    //    性能比较差， 能不用尽量不用， 可能有shuffle 可能有笛卡尔乘积；

    val joinRDD: RDD[(String, (Int, Int))] = rdd1.join(rdd2)
    joinRDD.collect().foreach(println)

    /*  RES
    (a,(4,1))
    (a,(4,4))
    (a,(3,1))
    (a,(3,4))
    (c,(1,9))

    */
```

##### 左右连接与cogroup

1. `val join3: RDD[(String, (Iterable[Int], Iterable[Int]))] = rdd1.cogroup(rdd2)`

```scala
    val rdd1: RDD[(String, Int)] = sc.makeRDD(
      List(
        ("a", 1), ("b", 2), ("c", 3), ("d", 4)
      )
    )

    val rdd2: RDD[(String, Int)] = sc.makeRDD(
      List(
        ("a", 5), ("b", 6), ("c", 7), ("d", 8)
      )
    )


    val joinRDD: RDD[(String, (Int, Option[Int]))] = rdd1.leftOuterJoin(rdd2)
    val rightjoin: RDD[(String, (Option[Int], Int))] = rdd1.rightOuterJoin(rdd2)

    val fullJoinRDD: RDD[(String, (Option[Int], Option[Int]))] = rdd1.fullOuterJoin(rdd2)

    // cogroup: group + connection
    val join3: RDD[(String, (Iterable[Int], Iterable[Int]))] = rdd1.cogroup(rdd2)

    /*
    (a,(CompactBuffer(1),CompactBuffer(5)))
    (b,(CompactBuffer(2),CompactBuffer(6)))
    (c,(CompactBuffer(3),CompactBuffer(7)))
    (d,(CompactBuffer(4),CompactBuffer(8)))

    */


    // (a,(CompactBuffer(1, 2, 3, 4),CompactBuffer(5, 6, 7, 8)))
    join3.collect().foreach(println)
    //    rightjoin.collect().foreach(println)
```

##### collect：行动算子

![1658276287940](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658276287940.png)

![1658276300850](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658276300850.png)

1. 懒加载，转换算子只是增加功能， 只有在执行行动算子的时候才会真正的去执行代码

```scala
    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1: RDD[Int] = sc.makeRDD(
      List(1, 2, 3, 4)
    )

    // map 算子不能让RDD执行
    // lazy， 用到了函数的结果的时候，才会执行函数， 不使用 函数， 函数不执行， 延迟加载；
    val newRDD: RDD[Int] = rdd1.map(
      num => {
        println("------------")
        num * 2
      }
    )

    // TODO 行动算子会触发jod的执行， 执行一次行动算子， 会触发一个新的job
    newRDD.collect()
```

##### 行动算子与转换算子的区别

1. 行动算子会启动 job 让任务真正的去执行

```scala
    //2.创建SparkContext，该对象是提交Spark App的入口

    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1: RDD[Int] = sc.makeRDD(
      List(1, 2, 3, 4)
    )
    //  TODO 转换算子功能：实现功能的扩和组合，
    //      返回结果一定是RDD；
    val mapRDD: RDD[Int] = rdd1.map(num => {
      num * 2
    })

    //  TODO 行动算子的功能： 触发job的执行， 应该有结果；

    val ints: Array[Int] = mapRDD.collect()
```

##### 转换算子与行动算子的区分

1. 行动算子返回的是一个结果

```scala
    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1: RDD[Int] = sc.makeRDD(
      List(1, 2, 3, 4),
      2
    )
    // Collect:返回结果是一个数据
    //  返回结果是一个数组,(不传参默认是全采集， 有参数是一个偏函数，是转换算子)
    //    应该存储在 Driver端内存在中;
    //  ------画图计算原理--Spark-算子-3;
    rdd1.collect()

```

2. `rdd1.collect()`: 会将计算的结果从执行节点 `Excute` 节点上收集到 `Driver` 当中

![1658228318433](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658228318433.png)

![1658228332476](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658228332476.png)

##### 几个常见行动算子

1. count，  first，  take，  
2. `rdd1.takeOrdered(3)`： 先排好序再取出

```scala
    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1: RDD[Int] = sc.makeRDD(
      List(1, 2, 3, 4),
      2
    )

    val l: Long = rdd1.count()
    val i: Int = rdd1.first()
    val ints: Array[Int] = rdd1.take(3)

    // takeOrdered： 排序完成之后 再取前几个
    val ints1: Array[Int] = rdd1.takeOrdered(3)
```

##### countByKey与countByValue实现WordCount

1. 使用 `map` 转换数据的结构

```scala
    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1: RDD[(String, Int)] = sc.makeRDD(
      List(
        ("a", 1), ("a", 2),
        ("b", 1), ("b", 2),
        ("b", 1), ("b", 2)
      ),
      2
    )

    // TODO  countByKey: 可以实现WordCount （7 / 10）
    // rdd1.countByKey().foreach(println)

    // val rdd12: collection.Map[String, Long] = rdd1.countByKey()


    /*
    (b,4)
    (a,2)
    */


    // TODO countByValue: 每个集合中的元素出现的次数    （8  /  10）
    //    rdd1.countByValue().foreach(println)

    // val rdd13: collection.Map[(String, Int), Long] = rdd1.countByValue()
```

##### aggregate与aggregateByKey

1. `aggregate`： 初始值在分区之间也要进行操作计算，
   1. 在各个分区之内参与运算，计算出结果之后
   2. 在分区之间进行计算，分区之间的计算属于一次操作，只对初始值参与计算一次

```scala
    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1 = sc.makeRDD(
      List(
        1, 2, 3, 4
      ),
      3
    )

    /*    val i: Int = rdd1.reduce(_+_)

        println(i)
        */

    // aggregate: 初始值在分区之间也要进行计算
    // aggregateByKey ： 只在分区内参加运算;
    val i: Int = rdd1.aggregate(10)(_+_, _+_)
    val i1: Int = rdd1.fold(10)(_+_)
    println(i)
```

##### 三种word Count

```scala
    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1: RDD[(String, Int)] = sc.makeRDD(
      List(
        ("a", 1), ("B", 1),
        ("a", 2), ("B", 3),
        ("a", 4), ("B", 7)
      ),
      2
    )

    //
    val mapRDD: RDD[mutable.Map[String, Int]] = rdd1.map(
      kv => {
        val stringToInt: mutable.Map[String, Int] = mutable.Map(kv)
        stringToInt
      }
    )

    // TODO fold算子可以实现 WordCount ( 11 / 10 )
    val rdd15: mutable.Map[String, Int] = mapRDD.fold(mutable.Map[String, Int]())(
      (map1, map2) => {
        map2.foreach {
          case (word, cnt) => {
            val oldCnt: Int = map1.getOrElse(word, 0)
            map1.update(word, oldCnt + cnt)
          }
        }
        map1
      }
    )

    // TODO aggregate 实现WordCount；（10  /  10）
	// 初始值使用一个 (map, kv)一个初始值map 与 元素中的二元组相加，
    val rdd14: mutable.Map[String, Int] = rdd1.aggregate(mutable.Map[String, Int]())(
      (map, kv) => {
        val oldCnt: Int = map.getOrElse(kv._1, 0)
        map.update(kv._1, oldCnt + kv._2)
        map
      },
      (map1, map2) => {
        map2.foreach {
          case (word, cnt) => {
            val oldCnt: Int = map1.getOrElse(word, 0)
            map1.update(word, oldCnt + cnt)
          }
        }
        map1
      }
    )

    // TODO reduce: 实现WordCount （9/ 10）
	// reduce map类型的数据， 相同的key进行聚合，是两个map进行操作
    val wordCount12: mutable.Map[String, Int] = mapRDD.reduce(
      (map1, map2) => {

        map2.foreach {
          case (word, cnt) => {
            val oldCnt: Int = map1.getOrElse(word, 0)
            map1.update(word, oldCnt + cnt)
          }
        }
        map1
      }
    )

    println(wordCount12)
```



##### 文件序列化

![1658276320822](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658276320822.png)

```scala
    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1 = sc.makeRDD(
      List(
        ("a", 1), ("b", 2), ("c", 3),
        ("a", 4), ("b", 5), ("c", 6)
      ),
      2
    )

    // 分区保存文件

    rdd1.saveAsTextFile("output1")

    // saveAsObjectFile: 序列化方式存储
    rdd1.saveAsObjectFile("output2")


    // saveAsSequenceFile: 必须使用 kv 类型数据
    rdd1.saveAsSequenceFile("output3")
```

##### 算子与函数 `foreach`

```scala
    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1 = sc.makeRDD(
      List(
        ("a", 1), ("b", 2), ("c", 3),
        ("a", 4), ("b", 5), ("c", 6)
      ),
      2
    )

    // 是 Array 的方法， 是单点的，
    rdd1.collect().foreach(println)

    println("--------------------")

    //这是Spark的算子， 分布式的， 是RDD的算子
    rdd1.foreach(println)
```

##### 对象的序列化相关

1. 算子的内部使用到外部变量或者属性的时候
2. 分布式计算，真正的计算过程在各个 `Excuter`  中，new的对象在Driver中，用到的时候需要序列化之后传过去

```scala
  def main(args: Array[String]): Unit = {
    //1.创建SparkConf并设置App名称
    val conf: SparkConf = new SparkConf().setAppName("SparkCoreTest").setMaster("local[*]")

    //2.创建SparkContext，该对象是提交Spark App的入口

    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1 = sc.makeRDD(
      List(
        1, 2, 3, 4
      ),
      2
    )

    // TODO 算子：
    //  算子的内部运行逻辑在Excuter端
    //  算子的外部在Driver端
    //  如果算子内部使用了算子外部的变量，需要序列化;
    //  ---------Spark-算子=-03 画图
    //  当前位置是Driver端 ；
    val user1 = new User

    //    forezch 在Excuter中运行
    rdd1.foreach(
      num => {
        println(user1.age + num)
      }
    )


    //4.关闭连接
    sc.stop()
  }

  case class User() {
    val age: Int = 30
  }

```

##### 闭包自动检测

1. 在调用算子的时候，会自动判断是否存在闭包
2. 如果存在闭包会自动检查是否序列化

```scala
    //2.创建SparkContext，该对象是提交Spark App的入口

    // TODO  行动算子
    val sc: SparkContext = new SparkContext(conf)

    val rdd1 = sc.makeRDD(
      List[Int]()
    )

    val user1 = new User

    //  TODO 闭包检测功能，
    //    1.判断是否存在闭包，
    //    2. 如果是闭包操作，会检查能否序列化；
    rdd1.foreach(
      num => {
        println(user1.age + num)
      }
    )

```

##### 依赖之间的关系

```scala
    //2.创建SparkContext，该对象是提交Spark App的入口
    val sc: SparkContext = new SparkContext(conf)


    val rdd1= sc.makeRDD(
      List(
        "Hello Scala", "Hello Spark"
      ),
      2
    )
    val rdd2: RDD[String] = rdd1.flatMap(_.split(" "))
    println(rdd2.toDebugString)
    println("----------------")

    val rdd3: RDD[(String, Int)] = rdd2.map((_, 1))
    println(rdd3.toDebugString)
    println("-------------------")

    val rdd4: RDD[(String, Int)] = rdd3.reduceByKey(_+_)
    println(rdd4.toDebugString)
    println("------------------")



    // 依赖 -------------画图-------------
    // 1. 新的RDD依赖于旧的RDD， 两者存在依赖关系
    // 2. 连续的依赖关系，称之为血缘关系，（间接依赖）；
```

![1658229779853](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658229779853.png)

- 当在某一步执行操作的时候发生意外，机器挂掉了，需要能够向前回溯，找到初始点，重新对数据进行操作
- 独享依赖（窄依赖）与 shuffle依赖（宽依赖）

![1658229911976](https://gitee.com/chnpngwng/typora-image/raw/master/assets/note/1658229911976.png)

