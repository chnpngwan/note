# sql练习

##### 1294

```sql
-- 返回不同国家11月的天气状况, 天气状况根据11月weather_state决定
-- avg(weather_state)<=15   寒冷
-- 15<avg(weather_state)<25   温暖
-- avg(weather_state)>=25   炎热
DROP table IF exists Countries;
DROP table IF exists Weather;
Create table Countries (country_id int, country_name string);
Create table Weather (country_id int, weather_state int, day date);
insert into Countries (country_id, country_name) values 
('2', 'USA')
,('3', 'Australia')
,('7', 'Peru')
,('5', 'China')
,('8', 'Morocco')
,('9', 'Spain');
insert into Weather (country_id, weather_state, day) values 
('2', '15', '2019-11-01')
,('2', '12', '2019-10-28')
,('2', '12', '2019-10-27')
,('3', '-2', '2019-11-10')
,('3', '0', '2019-11-11')
,('3', '3', '2019-11-12')
,('5', '16', '2019-11-07')
,('5', '18', '2019-11-09')
,('5', '21', '2019-11-23')
,('7', '25', '2019-11-28')
,('7', '22', '2019-12-01')
,('7', '20', '2019-12-02')
,('8', '25', '2019-11-05')
,('8', '27', '2019-11-15')
,('8', '31', '2019-11-25')
,('9', '7', '2019-10-23')
,('9', '3', '2019-12-23');


-- 首先要过滤出11月分所有的weather
select c.country_name,
       case when avg(weather_state)<=15 then '寒冷'
            when avg(weather_state)>15 and avg(weather_state)<25 then '温暖'
            else '炎热' end
from weather w
  join countries c on w.country_id = c.country_id
where SUBSTRing(w.day, 1, 7) = '2019-11'
group by c.country_name;

-- 套子查询的版本
SELECT country_name,
       case when avg_ws<=15 then '寒冷'
            when avg_ws>15 and avg_ws<25 then '温暖'
            else '炎热' end
from countries c join 
(
  select country_id, avg(weather_state) avg_ws
  from weather
  where SUBSTRing(day, 1, 7) = '2019-11'
  group by country_id
) t1
on c.country_id = t1.country_id;
```

##### 开窗练习

```sql
-- 开窗练习
drop table if EXISTS sales;
CREATE TABLE sales(
id            string,       --商品ID
category_id   string,       --商品分类ID
sales_sum     int           --商品销售数量
);

insert overwrite table sales
select  pos, 
        floor(rand(1231239819)*9) + 1, 
        floor(rand(1231457656)*10000) + 100
from (
  select posexplode(split(repeat("a",50000),"a"))
) t1
where pos <> 0;

--1. 每个品类消费排名前十
-- category_id, id, rank
--2. 各品类销量前十的商品列表以及销售总额
-- category_id, ids, total_sum



--1. 每个品类消费排名前十
-- category_id, id, rank

-- 首先先进行rank
SELECT id, category_id, sales_sum,
       rank() over (partition by category_id order by sales_sum DESC) rnk
FROM sales;
-- 过滤前10
SELECT category_id, id, rnk
from (
  SELECT id, category_id, sales_sum,
         rank() over (partition by category_id order by sales_sum DESC) rnk
  FROM sales
) t1
where rnk<=10;




--2. 各品类 销量前十的商品列表 以及 销售总额
-- category_id, ids, total_sum

-- 求前十总额
SELECT category_id,
       collect_list(id) ids,
       sum(sales_sum) total_sum
from (
  SELECT id, category_id, sales_sum,
         rank() over (partition by category_id order by sales_sum DESC) rnk
  FROM sales
) t1
where rnk<=10
group by category_id;

-- 求所有总额
SELECT category_id,
       collect_list(if(rnk<=10, id, null)) ids,
       sum(sales_sum) total_sum
from (
  SELECT id, category_id, sales_sum,
         rank() over (partition by category_id order by sales_sum DESC) rnk
  FROM sales
) t1
group by category_id;
```

# 窗口函数

- first_value/last_value

  ```sql
  -- first_value(col, boolean) over(有序窗口)
  -- boolean为false, 返回这个窗口中col的第一行，boolean为true，返回这个窗口中col不为null的第一行
  
  -- last_value(col, boolean) over(有序窗口)
  -- boolean为false, 返回这个窗口中col的最后一行，boolean为true，返回这个窗口中col不为null的最后一行
  ```

  ```sql
  -- 查询business表明细，以及截至消费当日每人的第一笔和最后一笔大于50元的消费日期
  select name,
         orderdate,
         cost,
         if(cost>50, orderdate, null),
         first_value(if(cost>50, orderdate, null), true) over(partition by name order by orderdate
                     rows between unbounded preceding and current row) fir,
         last_value(if(cost>50, orderdate, null), true) over(partition by name order by orderdate
                     rows between unbounded preceding and current row) las
  from business;
  
  -- 查询business表明细，以及截至消费当日每人的最近三笔消费中 第一笔大于50元的消费日期
  select name,
         orderdate,
         cost,
         if(cost>50, orderdate, null),
         first_value(if(cost>50, orderdate, null), true) over(partition by name order by orderdate
                     rows between 2 preceding and current row) fir
  from business;
  ```

  ## 其他常用函数

  - 日期相关函数

    ```sql
  -- current_date()            返回当前日期
    -- date_add(date, n)         返回从date开始n天之后的日期
  -- date_sub(date, n)         返回从date开始n天之前的日期
    -- datediff(date1, date2)    返回date1-date2的日期差
    ```
  
    ```sql
    -- year(date)                返回日期的年份
    -- month(date)               返回日期的月份
  -- day(date)                 返回日期的日
    -- dayofweek(date)           返回星期几（星期日是第一天）
    -- weekofyear(date)          返回今天数据第几周
    ```
  
  - 取整相关函数
  
    | 函数声名   | 解释     |
  | ---------- | -------- |
    | ceil(num)  | 向上取整 |
  | floor(num) | 向下取整 |
    | round(num) | 四舍五入 |
  
  - 复杂类型包装函数
  
    | 函数声名                                     | 解释                                                         |
    | -------------------------------------------- | ------------------------------------------------------------ |
    | str_to_map(str, field_sep, kv_sep)           | 将str按照field_sep分段，每段以kv_sep分成key value，返回一个map |
    | named_struct(name1, col1, name2, col2, ....) | 将col2，col2，....包装为结构体，名字为name1，name2，.....    |
  
    ```sql
    -- str_to_map
    select str_to_map('a:b,c:d,e:f', ',', ':');
    
    -- named_struct
    select named_struct(
        'name', name,
        'orderdate', orderdate,
        'cost', cost
    ) from business;
    ```
  
  6 . 自定函数的Jar报使用
  
    - 将jar包上传到/opt/module/hive/lib

    - 重启hiveserver2或者在客户端中热添加jar包

      ```bash
      
      ```
    # 重启hiveserver2
      hive_services.sh restart
    ```
  
      或者
  
      ```sql
    # 在beeline中执行以下命令进行热添加
      add jar /opt/module/hive/lib/hiveplugin220411-1.0-SNAPSHOT.jar;
    ```
  
    - 在Hive中常见函数与jar包关联
  
      ```sql
      create function mylen as 'com.atguigu.hive.MyUDF';
      ```
  
    - 如果函数不想要了
  
      ```sql
      drop function mylen;
      ```


# 分区和分桶

## 分区表

```sql
-- 创建分区表
-- 分区表就是把一张大表分若干个文件夹去管理
-- partitioned： 分割的
create table dept_partition(
    deptno int,
    dname string,
    loc int
)
partitioned by (dt string)
row format delimited fields terminated by '\t';
```

```sql
-- 向分区表中插入数据(load)
load data local inpath '/opt/module/hive/datas/dept.txt' 
into table dept_partition partition(dt='2022-06-14');

-- 插入数据(insert)
insert overwrite table dept_partition partition(dt='2022-06-17')
select deptno, dname, loc from dept;

-- 另外的写法
insert overwrite table dept_partition
select deptno, dname, loc, '2022-06-18' from dept;
```

```sql
-- 查询分区表分区数
show partitions dept_partition;
-- 分区过滤查询
select * from dept_partition where dt='2022-06-14';
```

```sql
-- 直接对分区表的分区进行操作
alter table dept_partition add partition(dt='2022-06-19');
alter table dept_partition add partition(dt='2022-06-20') partition(dt='2022-06-21');
-- 删除分区
alter table dept_partition drop partition(dt='2022-06-19');
alter table dept_partition drop partition(dt='2022-06-20'),partition(dt='2022-06-21');
```

```sql
-- 直接在分区表文件夹中新建对应分区
hadoop fs -cp /user/hive/warehouse/dept_partition/dt=2022-06-18 /user/hive/warehouse/dept_partition/dt=2022-06-13

-- 直接在分区表数据目录里面建立文件夹，分区表不能直接识别
-- 方案一；add partition
-- 方案二：官方修复分区表的命令
-- repair： 修理
msck repair table dept_partition;
```

```sql
-- 二级分区表
create table dept_partition2(
    deptno int,
    dname string,
    loc int
)
partitioned by (month string, day string)
row format delimited fields terminated by '\t';

-- 插入数据
insert into dept_partition2
select deptno, dname, loc, '2022-06','14' from dept;
-- load, 指定分区时需要指定所有级别的分区
load data local inpath '/opt/module/hive/datas/dept.txt' 
into table dept_partition2 partition(month='2022-06', day='15');
```

```sql
-- 动态分区
-- 普通的表格无法直接转化为分区表，只能先新建新的分区表，再将旧数据插入这个新的分区表
-- 例：将emp表转化为按照deptno分区的表格
create table emp_par(
    empno int,
    ename string,
    job string,
    salary decimal(16,2)
) partitioned by (deptno int)
row format delimited fields terminated by '\t';

-- 然后将数据插入这张分区表
-- 方法A：一个分区一个分区插过去
insert into emp_par partition(deptno=10)
select empno,ename,job,sal from emp where deptno=10;

-- 方法B：动态分区一次搞定
insert overwrite table emp_par
select empno,ename,job,sal,deptno from emp;
-- 动态分区时需要注意自动生成的分区数是有上限的
hive.exec.max.dynamic.partitions=1000           -- 全局动态分区上限
hive.exec.max.dynamic.partitions.pernode=100    -- 每个节点动态分区上限
hive.exec.max.created.files=100000              -- 创建的最大文件数量
hive.error.on.empty.partition=false             -- 空分区不抛异常
```

- 一般数据量很大的表格，我们要按照分区存储
- 分区表按照分区过滤时，可以减小数据扫描量
- 一般来说，在生产环境中，只要是分区表，尽量不要做全表扫描

## 分桶表

```sql
-- 新建分桶表
-- clustered： 成群的
create table stu_buck(
    id int,
    name string
)
clustered by (id) sorted by (id) into 4 buckets
row format delimited fields terminated by '\t';
```

```sql
-- 向分桶表中插入数据, 一般不要用load，如果要用，也要用HDFS的输入文件
load data inpath '/datas/student.txt' into table stu_buck;

-- 如果load本地数据源，由于分桶表插入数据要跑MR，有可能跑MapTask的节点本地没数据

-- 用insert插入
insert overwrite table stu_buck
select id,name from stu_ex;
```

```sql
-- 既分区也分桶
create table stu_par_buck(
    id int,
    name string
)
partitioned by (dt string)
clustered by (id) sorted by (id desc) into 4 buckets
row format delimited fields terminated by '\t';

-- 插入数据
insert into stu_par_buck
select id, name, '2022-06-14' from stu_ex;
```











































